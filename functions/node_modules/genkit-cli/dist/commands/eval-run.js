"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.evalRun = void 0;
const eval_1 = require("@genkit-ai/tools-common/eval");
const utils_1 = require("@genkit-ai/tools-common/utils");
const commander_1 = require("commander");
const promises_1 = require("fs/promises");
const manager_utils_1 = require("../utils/manager-utils");
exports.evalRun = new commander_1.Command('eval:run')
    .description('evaluate provided dataset against configured evaluators')
    .argument('<dataset>', 'Dataset to evaluate on (currently only supports JSON)')
    .option('--output <filename>', 'name of the output file to write evaluation results. Defaults to json output.')
    .option('--output-format <format>', 'The output file format (csv, json)', 'json')
    .option('--evaluators <evaluators>', 'comma separated list of evaluators to use (by default uses all)')
    .option('--force', 'Automatically accept all interactive prompts')
    .action(async (dataset, options) => {
    await (0, manager_utils_1.runWithManager)(async (manager) => {
        if (!dataset) {
            throw new Error('No input data passed. Specify input data using [data] argument');
        }
        let evaluatorActions;
        if (!options.evaluators) {
            evaluatorActions = await (0, eval_1.getAllEvaluatorActions)(manager);
        }
        else {
            const evalActionKeys = options.evaluators
                .split(',')
                .map((k) => `/evaluator/${k}`);
            evaluatorActions = await (0, eval_1.getMatchingEvaluatorActions)(manager, evalActionKeys);
        }
        utils_1.logger.info(`Using evaluators: ${evaluatorActions.map((action) => action.name).join(',')}`);
        if (!options.force) {
            const confirmed = await (0, utils_1.confirmLlmUse)(evaluatorActions);
            if (!confirmed) {
                if (!confirmed) {
                    throw new Error('User declined using billed evaluators.');
                }
            }
        }
        const evalDataset = JSON.parse((await (0, promises_1.readFile)(dataset)).toString('utf-8')).map((testCase) => ({
            ...testCase,
            testCaseId: testCase.testCaseId || (0, utils_1.generateTestCaseId)(),
            traceIds: testCase.traceIds || [],
        }));
        const evalRun = await (0, eval_1.runEvaluation)({
            manager,
            evaluatorActions,
            evalDataset,
        });
        if (options.output) {
            const exportFn = (0, eval_1.getExporterForString)(options.outputFormat);
            await exportFn(evalRun, options.output);
        }
        console.log(`Succesfully ran evaluation, with evalId: ${evalRun.key.evalRunId}`);
    });
});
//# sourceMappingURL=eval-run.js.map